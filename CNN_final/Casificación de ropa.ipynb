
# Importo NumPy para trabajar con arreglos y datos numéricos
import numpy as np

# Importo TensorFlow para crear y entrenar mi red neuronal
import tensorflow as tf

# Importo clases y capas básicas para construir el modelo
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Softmax

# Importo el optimizador Adam, que me sirve para ajustar pesos
from tensorflow.keras.optimizers import Adam

# Importo el dataset fashion_mnist que trae imágenes de ropa (zapatos, playeras, etc.)
from tensorflow.keras.datasets import fashion_mnist

# Importo la función de pérdida que utilizaré (SparseCategoricalCrossentropy)
# esta se usa cuando las etiquetas NO están en one-hot
from tensorflow.keras.losses import SparseCategoricalCrossentropy

# Importo métricas para evaluar y graficar la matriz de confusión
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns

# Importo matplotlib para graficar resultados y visualizaciones
import matplotlib.pyplot as plt

fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()




# .load_data() es el método que realmente carga los datos del dataset Fashion MNIST
# cuando lo ejecuto, me regresa 2 tuplas:
# (X_train, y_train) → datos y etiquetas para entrenar
# (X_test, y_test)   → datos y etiquetas para prueba




# Defino el índice de la imagen que quiero visualizar del dataset
# aquí estoy eligiendo la posición 10 (es decir, la imagen número 11)
index = 10

# Define the class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']


image = X_train[index]   # toma la imagen número "index"
label = y_train[index]   # toma su etiqueta correspondiente
print(image)

# creo una figura con tamaño de 3x3 pulgadas
plt.figure(figsize=(3, 3))

# muestro la imagen en escala de grises
plt.imshow(image, cmap=plt.cm.gray)

# pongo el título de la imagen usando la etiqueta numérica y el nombre de la clase
plt.title(f'Etiqueta número: {label} es {class_names[label]}')

# finalmente muestro la imagen en pantalla
plt.show()
print(X_train.shape)
print(X_test.shape)


# imprimo la forma (shape) del conjunto de imágenes de entrenamiento
print(X_train.shape)

# imprimo la forma del conjunto de imágenes de prueba
print(X_test.shape)

# Normalizo las imágenes dividiendo todos los pixeles entre 255
# esto hace que los valores pasen de estar en un rango 0-255 (intensidad de gris)
# a estar en un rango 0-1, lo cual ayuda a que la red neuronal entrene mejor
X_train = X_train / 255.0
X_test = X_test / 255.0

# Creo un modelo CNN secuencial
model = Sequential([
    # Primera capa conv: 32 filtros de 3x3, función de activación ReLU
    # input_shape indica el tamaño de las imágenes: 28x28 y 1 canal (escala de grises)
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),

    # Reduzco tamaño con MaxPooling (tomando el valor máximo en ventanas 2x2)
    MaxPooling2D((2, 2)),

    # Segunda capa convolucional con 64 filtros, para captar más patrones
    Conv2D(64, (3, 3), activation='relu'),

    # Otra capa de pooling para seguir reduciendo dimensiones
    MaxPooling2D((2, 2)),

    # Otra capa conv con 64 filtros
    Conv2D(64, (3, 3), activation='relu'),

    # Aplano (flatten) el volumen para pasar a capas densas
    Flatten(),

    # Capa densa con 64 neuronas, activación relu
    Dense(64, activation='relu'),

    # Capa final: salida de 10 neuronas (10 clases de FashionMNIST)
    # aquí todavía no pongo activación softmax, se lo agrego luego al compilar
    Dense(10)
])



# Defino la tasa de aprendizaje que quiero usar para Adam
# (qué tan fuerte ajusta los pesos en cada paso)
learning_rate = 0.001

# Creo el optimizador Adam con esa tasa de aprendizaje
adam_optimizer = Adam(learning_rate=learning_rate)


# Compilo el modelo, es decir, aquí le digo cómo quiero que aprenda
model.compile(
    optimizer=adam_optimizer,                          # uso el optimizador Adam que definí antes
    loss=SparseCategoricalCrossentropy(from_logits=True),  # función de pérdida, mide qué tan mal se equivoca
    metrics=['accuracy']                               # quiero medir la exactitud durante el entrenamiento
)



# Entreno el modelo con mis datos de entrenamiento
history = model.fit(
    X_train, y_train,     # datos para entrenar
    epochs=10,                      # cuántas veces va a pasar por TODOS los datos
    validation_data=(X_test, y_test)  # datos que NO se entrenan, solo sirven para evaluar
)





# Evaluo mi modelo usando los datos de prueba (test)
# aquí ya no se entrena, solo se mide qué tan bien generaliza
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)

# test_loss = qué tanto se equivocó en las predicciones
# test_acc  = qué tan exacto fue en las predicciones (% bien)

# Creo un nuevo modelo que recibe el modelo original
# y le agrego un Softmax para convertir los logits en probabilidades
probability_model = Sequential([model, Softmax()])

# Con este modelo ya puedo predecir probabilidades reales (0 a 1)
predictions = probability_model.predict(X_test)

# De todas las probabilidades, tomo el índice que tiene el mayor valor
# ese índice corresponde a la clase predicha (0-9)
predicted_labels = np.argmax(predictions, axis=1)
# Calculo la matriz de confusión (qué predijo vs qué era en realidad)
cm = confusion_matrix(y_test, predicted_labels)

# Creo un objeto que me permitirá visualizar esa matriz
# y le paso los nombres de las clases para que salga más entendible
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)


# Graficar matriz de confusión
fig, ax = plt.subplots(figsize=(10, 10))
disp.plot(ax=ax)
plt.show()
