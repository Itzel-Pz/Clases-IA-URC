# -*- coding: utf-8 -*-
"""Predicción de popularidad de una canción.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HAlMD3J32MT0qPS0gdXgFAe6zl_SuOhB
"""

import pandas as pd

url = "https://raw.githubusercontent.com/mevangelista-alvarado/datasets/refs/heads/main/spotify_songs.csv"
df = pd.read_csv(url)

df

# Seleccionar características (features)
features = [
    'danceability', 'energy', 'key', 'loudness',
    'mode', 'speechiness', 'acousticness', 'instrumentalness',
    'liveness', 'valence', 'tempo', 'duration_ms',
]
X = df[features].values   # aquí tomo solo las columnas que elegí como características y las convierto a un arreglo numpy (X)

# Target numérico
y = df['popularity'].values   # aquí tomo la variable objetivo (popularity) que quiero predecir y la paso a arreglo numpy (y)

from sklearn.model_selection import train_test_split   # importo la función para dividir los datos en entrenamiento y prueba

X_train, X_test, y_train, y_test = train_test_split(   # divido mis datos en 4 partes: X para entrenamiento, X para prueba, y para entrenamiento y y para prueba
    X, y,                                              # X son las características y y es el target que quiero predecir
    test_size=0.2, random_state=42                     # uso 20% para prueba y fijo random_state para que la división sea reproducible
)

from sklearn.preprocessing import StandardScaler   # importo el escalador para normalizar las variables

scaler = StandardScaler()                          # creo una instancia del StandardScaler
X_train = scaler.fit_transform(X_train)            # ajusto el escalador con los datos de entrenamiento y los transformo (los normalizo)
X_test = scaler.transform(X_test)                  # transformo los datos de prueba usando el mismo escalador (sin volver a ajustar)

from tensorflow.keras.models import Sequential     # importo la clase Sequential para crear mi red neuronal capa por capa
from tensorflow.keras.layers import Dense          # importo la capa Dense, que es una capa totalmente conectada

model = Sequential([                               # defino mi modelo como secuencial (las capas van una después de otra)
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # primera capa densa con 64 neuronas y activación relu, especifico el número de features de entrada
    Dense(32, activation='relu'),                                # segunda capa densa con 32 neuronas y activación relu
    Dense(1, activation='linear')                                # capa de salida con 1 neurona (porque quiero predecir un valor numérico) y activación lineal
])

from tensorflow.keras.optimizers import Adam   # importo el optimizador Adam para entrenar mi modelo

# Tasa de aprendizaje deseada
learning_rate = 0.001                         # defino la tasa de aprendizaje que quiero usar (qué tan rápido aprende el modelo)
adam_optimizer = Adam(learning_rate=learning_rate)   # creo el optimizador Adam usando esa tasa de aprendizaje

model.compile(
    optimizer=adam_optimizer,   # le digo al modelo que va a entrenar usando el optimizador Adam que definí antes
    loss='mse',                 # uso MSE (error cuadrático medio) como función de pérdida porque esto es un problema de regresión
    metrics=['mae'],            # además quiero monitorear el MAE (error absoluto medio) para tener otra medida de desempeño
)

history = model.fit(
    X_train,       # datos de entrada para entrenar el modelo
    y_train,       # valores reales (target) correspondientes a X_train
    validation_split=0.2,  # tomo el 20% del entrenamiento para validación interna (sin tocar X_test todavía)
    epochs=50,     # número de veces que el modelo verá TODOS los datos completos
    batch_size=50, # cada iteración entrena con 50 ejemplos antes de actualizar los pesos
)

import matplotlib.pyplot as plt   # importo matplotlib para poder graficar

plt.plot(history.history['loss'], label='Pérdida de entrenamiento')       # grafico la pérdida del entrenamiento por cada época
plt.plot(history.history['val_loss'], label='Pérdida de validación')      # grafico la pérdida en los datos de validación por época
plt.xlabel('Épocas')                                                     # nombre para el eje X (epocas de entrenamiento)
plt.ylabel('Pérdida')                                                     # nombre para el eje Y (valor de pérdida)
plt.legend()                                                              # muestro la leyenda para diferenciar ambas curvas
plt.title('Función de pérdida durante el entrenamiento')                  # título de la gráfica
plt.show()                                                                # muestro la gráfica en pantalla

loss, mae = model.evaluate(X_test, y_test)   # evalúo el modelo con los datos de prueba y obtengo la pérdida y el MAE
print(f"MAE en el conjunto test: {mae}")     # imprimo el MAE para ver qué tan bien predice el modelo en datos no vistos

import pandas as pd   # importo pandas para manipular datos en formato de tabla

predictions = model.predict(X_test)                      # genero las predicciones del modelo usando el conjunto de prueba
comparison = pd.DataFrame({'Actual': y_test,             # creo un DataFrame para comparar
                           'Predicted': predictions.flatten()})  # meto los valores reales y los predichos (los aplano para que sea 1D)
print(comparison.head())                                # muestro las primeras filas para ver un ejemplo de comparación

nombre_cancion = "Beso"   # aquí elijo una palabra clave o parte del nombre de la canción que quiero buscar

canciones_df = df[df['track_name'].str.contains(nombre_cancion, case=False, na=False)]
# filtro el DataFrame original y me quedo solo con las filas donde el nombre de la canción contiene la palabra "Beso"
# case=False -> ignora mayúsculas/minúsculas
# na=False   -> evita errores si hay valores null

print(f"Canciones encontradas:")  # imprimo un texto para indicar el resultado
canciones_df[['track_name', 'artists', 'album_name']].head()  # muestro las primeras coincidencias con columnas relevantes

# indice a selecionar
i = 0                                         # elijo el índice 0 (la primera canción en el DataFrame filtrado)

cancion = canciones_df.iloc[i]                # obtengo la fila de esa canción (sus datos completos)

X_input = cancion[features].values.reshape(1, -1)
# selecciono solo las features numéricas que usó el modelo para entrenar
# convierto a arreglo y reshape para que tenga forma (1, número_de_features)

X_input = scaler.transform(X_input)            # escalo esa canción con el mismo escalador usado durante el entrenamiento

prediccion = model.predict(X_input)[0][0]      # hago la predicción con el modelo y tomo el valor escalar de resultado

print(f"Canción: {cancion['track_name']} - {cancion['artists']}")         # muestro el nombre y artista de la canción seleccionada
print(f"Popularidad real: {cancion['popularity']}")                      # muestro su popularidad real
print(f"Predicción: {prediccion:.2f}")                                    # muestro la predicción del modelo, formateada a 2 decimales

from sklearn.metrics import r2_score, mean_squared_error   # importo métricas para evaluar modelos de regresión

r2 = r2_score(y_test, predictions)          # calculo el coeficiente R², que mide qué tanto explica mi modelo la variabilidad real
print(f'R²: {r2}')                           # imprimo el valor de R²

mse = mean_squared_error(y_test, predictions)  # calculo el error cuadrático medio (MSE), qué tan lejos están las predicciones del valor real
print(f'MSE: {mse}')                           # imprimo el MSE